# 大数据

### 应用场景

大数据相关的场景比较多，常见的有：
- ETL（数据提取、转换、加载）、
- 实时流式（监控报警、风控等）、
- 机器学习（推荐引擎、用户画像、风控安全等）、
- 非结构化分析（视频、图片、语音、文本等）、
- 海量大数据在线存储（HBase）、搜索及OLAP。
- 其中OLAP（在线联机分析）在很多企业占住分析类的大部分。

https://yq.aliyun.com/articles/198435

### 资料

- 大数据产品列表 https://zhuanlan.zhihu.com/p/21752798
- 数据仓库历史 https://zhuanlan.zhihu.com/p/26815743

### 数据源

分布式数据源的发展方向是统一的数据接口？SQL + UDF？

- 数据（考虑各种数据源的支持）
  - 单机数据库（分表分库是第一步，参考MyCAT/ActorDB）
  - NoSQL（KV类，参考tispark）
  - 分布式数据库
- 文件源（HADOOP）
- 流式日志
- 海量小文件系统：文档（文本、WORD、PDF、HTML、报表）
- 图像/音频/视频

### 组件套餐

- collector & etl
  - sqoop
  - flume
  - spider
  - data gateway
  - canal
- hbase/hbase-indexer/hadoop
- spark
  - spark sql jdbc/hbase rdd
  - spark hbase connector
  - deeplearning4j
  - Oozie ?
- zeppelin/superset/saiku
- kafka/gobbin -> hdfs?
- spring boot
- nagios?

**常见的开发场景**

- 数据接入
- hbase索引优化
- spark算法及实现
- 界面定制开发
- 应用接口

### 日志型组件套餐 ^ ^

- logstash
- cassandra & index plugin
- presto

### 收集的一些github项目

- openrefine
- extraction-framework
- webmagic
- kairos
- indexr
- ksql for kafka
- lily hbase indexer https://github.com/NGDATA/hbase-indexer
- nifty, thrift client/server
- boilerpipe, html extractor
- hanlp, corenlp, mallet

### 实施的坑

- 操作系统redhat/centos，注意要上raid(raid2, raid5?)，防止存储出问题
- 存储是否要上集中式存储，磁盘阵列或其他？
- 采用成熟的套件，cloudera/CDH?
- 需要对平台进行监控，配合成熟的监控工具？

### 数据中心

- 数据采集、清洗、整合
  - 模板化主动采集、清洗、过滤、转换（模板化有助于检测数据的样式）
  - 支持多种协议：WS/FTP/JDBC/...
  - 传输过程的加密？是否必要？通信白名单？
  - 实时监控、采集异常及时处理
  - 完整性、合理性校验，提供数据质量检验报告
  - 过程监督，进度报告
  - 结构化，半结构化，非结构化数据
  - 采集的时候，数据分散，格式不一，如何保证入库的完整完备？
- 数据存储：业务相关，合理分库分表，考虑上层计算以及海量存储的要求
- 数据分析与服务层：依赖OLAP引擎，提供REST接口？
- 应用服务层：根据具体业务，提供在OLAP基础上，各种需要的监测功能展示（BI层）
- 数据治理
  - 事前标准制定、事中过程监管与质量评估、事后质量提升的完整治理体系
  - 提供元数据管理、主索引管理、数据质量管理、数据服务管理、授权管理、安全管理、监控管理能力

### ETL

数据的采集，有主动和被动两种，需要面对数据源多变及通信状况多变的问题：

完整的数据采集环节，需要考虑：
- 多级的处理手段（主动和被动都可用）
- 通用的处理方法（模板化）
- 多级的信息处理链，方便重用处理的插件
- 支持分布式架构，具备海量的处理能力
- 采集出现问题的时候需要报警（可能是对方的接口版本发生了变化，或者是网络状况出现了问题）
- 采集回来的数据，需要进行验证，包括单条数据验证和多条数据验证
- 数据问题要出异常报告，定时反馈给涉及多方，持续保证数据质量
- 需要保留采集回来的数据原样，在原数据的基础上，再做数据转换处理（考虑转换处理出错的情况）
- 数据转换处理模型需要合理设计，并且与业务多方进行协调沟通

**特别小心**

采用笔录的方式（非电子化采集）获得的数据，需要使用机器学习的方法，才能有效提取，无效信息过多

**团队合作**
- 有人写一个通用的数据导出工具，可以用java，可以用脚本，或其他的工具，总之要通用，可以通过不同的脚本文件来控制，使各地区的不同数据库导出的文件格式是一样的。而且还可以实现并行操作。
- 有人写FTP的程序，可以用bat，可以用ETL工具，可以用其他的方式，总之要准确，而且方便调用和控制。
- 有人设计数据模型，包括在1之后导出的结构，还有ODS和DWH中的表结构。
- 有人写SP，包括ETL中需要用到的SP还有日常维护系统的SP，比如检查数据质量之类的。
- 有人分析原数据，包括表结构，数据质量，空值还有业务逻辑。
- 有人负责开发流程，包括实现各种功能，还有日志的记录等等。
- 有人测试真正好的ETL，都是团队来完成的，一个人的力量是有限的。

* 数据清洗：
  - 数据补缺：对空数据、缺失数据进行数据补缺操作，无法处理的做标记。
  - 数据替换：对无效数据进行数据的替换。
  - 格式规范化：将源数据抽取的数据格式转换成为便于进入仓库处理的目标数据格式。
  - 主外键约束：通过建立主外键约束，对非法数据进行数据替换或导出到错误文件重新处理。
* 数据转换
    - 数据合并：多用表关联实现，大小表关联用lookup，大大表相交用join（每个字段家索引，保证关联查询的效率）
    - 数据拆分：按一定规则进行数据拆分
    - 行列互换、排序/修改序号、去除重复记录
    - 数据验证：loolup、sum、count
  - 实现方式：
    - 在ETL引擎中进行（SQL无法实现的）
    - 在数据库中进行（SQL可以实现的）
* 数据加载
  - 方式：
    - 时间戳方式：在业务表中统一添加字段作为时间戳，当OLAP系统更新修改业务数据时，同时修改时间戳字段值。
    - 日志表方式：在OLAP系统中添加日志表，业务数据发生变化时，更新维护日志表内容。
    - 全表对比方式：抽取所有源数据，在更新目标表之前先根据主键和字段进行数据比对，有更新的进行update或insert。
    - 全表删除插入方式：删除目标表数据，将源数据全部插入。
  - 异常处理，在ETL的过程中，必不可少的要面临数据异常的问题，处理办法：
    - 将错误信息单独输出，继续执行ETL，错误数据修改后再单独加载。中断ETL，修改后重新执行ETL。原则：最大限度接收数据。
    - 对于网络中断等外部原因造成的异常，设定尝试次数或尝试时间，超数或超时后，由外部人员手工干预。
    - 例如源数据结构改变、接口改变等异常状况，应进行同步后，在装载数据。

### 不同行业对数据处理的要求差异

开源与标准性是普遍关心的需求

- 互联网（教育）：低成本，可扩展性，
- 电商：安全性，可扩展性
- 互联网金融：安全性
- 游戏：低成本，高性能，可扩展性
- 政府服务：可靠性，口碑品牌，运维支持
- SaaS/企业方案：可扩展性，低成本
- 传统工业：简易，整体方案，接入接口丰富
- 企业信息化：方案功能全面，可扩展性好
